import colorsys
import glob
import os
import random

import matplotlib.pyplot as plt
import numpy as np
from PIL import Image, ImageOps

from main import NUMBER_SAMPLES, load, angle, length, height, ANGLE_PEAKS, NUMBER_DIVISIONS, OUTPUT_SIZE, FOLDER_ROOT, FOLDER_TRAIN_INPUTS, FOLDER_TRAIN_OUTPUTS, NUMBER_DIGITS, generate_input_images, calculate_load_components, write_ansys_script


# Names of files to be generated.
FILENAME_SAMPLES = 'train_samples.txt'

# Generate sample values for each parameter.
def generate_samples(show_histogram=False) -> tuple:
    # Helper function for generating unevenly spaced samples within a defined range. Setting "increasing" to True makes spacings increase as values increase.
    generate_logspace_samples = lambda low, high, increasing: (((
        np.logspace(
            0, 1, round(NUMBER_SAMPLES/4)+1
            ) - 1) / 9) * (1 if increasing else -1) + (0 if increasing else 1)
        ) * (high - low) + low
    # Generate samples.
    load_samples = np.linspace(load.low, load.high, NUMBER_SAMPLES)
    angle_samples = np.concatenate((
        generate_logspace_samples(angle.low, (ANGLE_PEAKS[1]-ANGLE_PEAKS[0])/2, increasing=True)[:-1],
        generate_logspace_samples((ANGLE_PEAKS[1]-ANGLE_PEAKS[0])/2, ANGLE_PEAKS[1], increasing=False)[1:],
        generate_logspace_samples(ANGLE_PEAKS[1], (ANGLE_PEAKS[1]-ANGLE_PEAKS[0])/2 + ANGLE_PEAKS[1], increasing=True)[:-1],
        generate_logspace_samples((ANGLE_PEAKS[1]-ANGLE_PEAKS[0])/2 + ANGLE_PEAKS[1], angle.high, increasing=False)[1:],
        ))
    length_samples = np.linspace(length.low, length.high, NUMBER_SAMPLES)
    height_samples = np.linspace(height.low, height.high, NUMBER_SAMPLES)
    # Randomize ordering of samples.
    np.random.shuffle(load_samples)
    np.random.shuffle(angle_samples)
    np.random.shuffle(length_samples)
    np.random.shuffle(height_samples)
    # Round samples to a fixed number of decimal places.
    load_samples = np.round(load_samples, load.precision)
    angle_samples = np.round(angle_samples, angle.precision)
    length_samples = np.round(length_samples, length.precision)
    height_samples = np.round(height_samples, height.precision)
    
    # Plot histograms for angle samples.
    if show_histogram:
        plt.figure()
        plt.hist(
            angle_samples,
            bins=round(angle.high-angle.low),
            rwidth=0.75,
            color='#0095ff',
            )
        plt.xticks(np.linspace(angle.low, angle.high, 5))
        plt.title(angle.name)
        plt.show()
    return load_samples, angle_samples, length_samples, height_samples

# Write the specified sample values to a text file.
def write_samples(samples, load_components) -> None:
    # Write samples to text file.
    text = [
        f'{str(i+1).zfill(NUMBER_DIGITS)},  Load: {load_sample:>10},  X load: {load_x:>15},  Y load: {load_y:>15},  Angle: {angle_sample:>10},  Length: {length_sample:>5},  Height: {height_sample:>5}\n'
        for i, (load_sample, angle_sample, length_sample, height_sample, (load_x, load_y)) in enumerate(zip(*samples, load_components))
        ]
    with open(os.path.join(FOLDER_ROOT, FILENAME_SAMPLES), 'w') as file:
        file.writelines(text)
    print(f'Wrote samples in {FILENAME_SAMPLES}.')

# Return the sample values found in the text file previously generated.
def read_samples() -> tuple:
    load_samples, angle_samples, length_samples, height_samples = [], [], [], []
    filename = os.path.join(FOLDER_ROOT, FILENAME_SAMPLES)
    try:
        with open(filename, 'r') as file:
            for line in file.readlines():
                load, *_, angle, length, height = [float(line.split(':')[1]) for line in line.split(',')[1:]]
                load_samples.append(load)
                angle_samples.append(angle)
                length_samples.append(length)
                height_samples.append(height)
    except FileNotFoundError:
        print(f'"{filename}" not found.')
        return
    else:
        print(f'Found samples in {filename}.')
        return load_samples, angle_samples, length_samples, height_samples

# Get properly ordered stress data in text files generated by FEA.
def convert_fea_to_spreadsheet(samples, output_filename):
    length_samples, height_samples = samples[2:4]
    filenames = glob.glob(os.path.join(FOLDER_TRAIN_OUTPUTS, '*.txt'))
    filenames = sorted(filenames)
    assert len(filenames) == NUMBER_SAMPLES, f'Found {len(filenames)} .txt files in {FOLDER_TRAIN_OUTPUTS}, but should be {NUMBER_SAMPLES}.'
    stresses = np.zeros((*OUTPUT_SIZE[::-1], len(filenames)))
    for i, filename in enumerate(filenames):
        with open(filename, 'r') as file:
            stress = [float(line) for line in file.readlines()]
        array = np.zeros(stresses.shape[:2])
        # Interior nodes.
        array[1:-1, 1:-1] = np.flipud(
            np.reshape(stress[2*sum(NUMBER_DIVISIONS):], [_-1 for _ in NUMBER_DIVISIONS[::-1]], 'F')
            )
        # Corner nodes.
        array[-1, 0] = stress[0]
        array[-1, -1] = stress[1]
        array[0, -1] = stress[1+NUMBER_DIVISIONS[0]]
        array[0, 0] = stress[1+NUMBER_DIVISIONS[0]+NUMBER_DIVISIONS[1]]
        # Edge nodes.
        array[-1, 1:-1] = stress[2:2+NUMBER_DIVISIONS[0]-1]
        array[1:-1, -1] = stress[2+NUMBER_DIVISIONS[0]:2+NUMBER_DIVISIONS[0]+NUMBER_DIVISIONS[1]-1][::-1]
        array[0, 1:-1] = stress[2+NUMBER_DIVISIONS[0]+NUMBER_DIVISIONS[1]:2+2*NUMBER_DIVISIONS[0]+NUMBER_DIVISIONS[1]-1][::-1]
        array[1:-1, 0] = stress[2+2*NUMBER_DIVISIONS[0]+NUMBER_DIVISIONS[1]:2+2*NUMBER_DIVISIONS[0]+2*NUMBER_DIVISIONS[1]-1]
        # Resize based on the size of the cantilever.
        array_scaled = array - np.min(array)
        array_scaled = array / np.max(array_scaled)
        with Image.fromarray((array_scaled*255).astype(np.uint8), 'L') as image:
            image = image.resize((
                round((length_samples[i] / length.high) * OUTPUT_SIZE[0]),
                round((height_samples[i] / height.high) * OUTPUT_SIZE[1]),
                ))
            array_scaled = np.asarray(image, float) / 255
            array_scaled *= max(stress) - min(stress)
            array_scaled += min(stress)
        # Insert the resized array.
        array[:] = 0
        array[:array_scaled.shape[0], :array_scaled.shape[1]] = array_scaled
        stresses[:, :, i] = array
    # Scale all values to [0, 1].
    stresses -= np.min(stresses)
    stresses /= np.max(stresses)
    # Write values to spreadsheet file.
    output_filepath = os.path.join(FOLDER_TRAIN_OUTPUTS, output_filename)
    stresses.flatten().tofile(
        output_filepath,
        sep=',',
        )
    print(f'Wrote properly ordered stress data from FEA in {output_filepath}.')

if __name__ == '__main__':
    # Try to read sample values from the text file if it already exists. If not, generate the samples.
    samples = read_samples()
    if not samples:
        samples = generate_samples(show_histogram=False)
    load_components = calculate_load_components(*samples[0:2])
    write_samples(samples, load_components)
    write_ansys_script(samples, load_components, 'ansys_script_train.lgw')
    generate_input_images(samples, FOLDER_TRAIN_INPUTS)
    convert_fea_to_spreadsheet(samples, 'stress.csv')
    # # Crop and resize stress contour images generated by FEA. This only needs to run the first time the images are added to the folder.
    # crop_output_images(samples)